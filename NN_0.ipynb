{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:19.360387Z",
     "start_time": "2024-09-17T14:30:19.357566Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### In this part we will see how a input data goes through the different layers (Forward Propagation only) of a neural network.\n",
    "\n",
    "#### The design of our network is:\n",
    "- linear input layer (Now if you want to know, why the input layer is linear, [check this thread](https://datascience.stackexchange.com/questions/73324/why-is-the-input-to-an-activation-function-a-linear-combination-of-the-input-fea))\n",
    "- 1*hidden layer with ReLU activation function ([check this link about activation functions, you will also find very informative blog posts](https://www.datacamp.com/tutorial/introduction-to-activation-functions-in-neural-networks))\n",
    "- Linear Layer\n",
    "- Softmax\n",
    "\n",
    "\n",
    "For overall, a better understanding:\n",
    "[ML_google_tutorial](https://developers.google.com/machine-learning/crash-course/neural-networks/nodes-hidden-layers)"
   ],
   "id": "d5238cc22ff171e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<b>Step-0:</b> Flattening before input\n",
    "\n",
    "*** Randomly generated input ***"
   ],
   "id": "c0e2db2662048f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:19.411754Z",
     "start_time": "2024-09-17T14:30:19.398929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## How Flatten works?\n",
    "X = torch.rand(3, 12, 12)\n",
    "print(\"before flattening:\\n\", X)\n",
    "print(X.size())\n",
    "\n",
    "## After Flattening\n",
    "flatten = nn.Flatten()\n",
    "flat_x = flatten(X)\n",
    "print(\"after flattening:\\n\", flat_x)\n",
    "print(flat_x.size())"
   ],
   "id": "4618d269e4309b8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before flattening:\n",
      " tensor([[[1.2036e-01, 3.5857e-01, 3.0722e-01, 6.0852e-01, 9.4305e-01,\n",
      "          2.7720e-01, 3.9667e-01, 4.7321e-01, 2.1927e-01, 9.6870e-01,\n",
      "          7.8115e-01, 9.9759e-01],\n",
      "         [7.1010e-01, 6.7218e-01, 7.5192e-01, 4.6336e-02, 2.4287e-01,\n",
      "          1.6409e-01, 8.3532e-01, 8.4018e-02, 1.8201e-01, 5.0194e-01,\n",
      "          2.3946e-01, 4.7157e-01],\n",
      "         [8.4941e-01, 8.7402e-01, 7.2627e-02, 3.0818e-02, 8.5465e-01,\n",
      "          6.8261e-01, 7.5643e-01, 2.6868e-01, 5.5701e-02, 3.1213e-01,\n",
      "          6.5913e-01, 6.4715e-01],\n",
      "         [1.4572e-01, 5.0676e-01, 9.9729e-01, 8.4374e-01, 5.6391e-01,\n",
      "          9.9580e-01, 2.0392e-01, 9.2289e-01, 7.8074e-01, 7.0441e-01,\n",
      "          9.6406e-01, 5.1472e-01],\n",
      "         [9.4192e-01, 1.2049e-01, 6.5736e-01, 2.2819e-02, 6.0878e-01,\n",
      "          3.2500e-01, 1.3713e-01, 6.0726e-01, 8.3132e-02, 1.7710e-01,\n",
      "          3.1849e-01, 3.0600e-01],\n",
      "         [8.2168e-01, 5.6157e-02, 9.0684e-01, 1.5835e-01, 9.7183e-01,\n",
      "          8.5267e-02, 6.8410e-01, 8.3593e-01, 6.9281e-01, 5.4662e-01,\n",
      "          8.3735e-01, 5.0868e-01],\n",
      "         [6.6205e-01, 2.8574e-01, 8.6120e-01, 4.1313e-01, 3.1117e-01,\n",
      "          1.5610e-01, 1.9948e-01, 9.3747e-01, 6.8569e-04, 4.7271e-02,\n",
      "          1.9549e-01, 8.5182e-03],\n",
      "         [3.9558e-01, 7.9935e-03, 3.3891e-01, 8.6581e-02, 6.7433e-01,\n",
      "          9.3007e-01, 1.7772e-01, 6.1982e-01, 2.4283e-01, 5.4227e-03,\n",
      "          1.7955e-01, 4.4270e-02],\n",
      "         [8.6561e-01, 8.4890e-01, 2.9216e-01, 2.9569e-01, 7.1616e-03,\n",
      "          5.2166e-01, 7.5104e-01, 2.2750e-01, 1.8223e-01, 7.3204e-01,\n",
      "          8.9928e-01, 9.9482e-01],\n",
      "         [9.0702e-01, 9.7863e-01, 8.2605e-01, 8.2432e-01, 7.9553e-01,\n",
      "          1.8847e-01, 5.1431e-02, 5.2477e-01, 4.1782e-01, 1.0863e-01,\n",
      "          3.8186e-01, 5.0454e-01],\n",
      "         [6.9354e-01, 3.3332e-01, 1.9284e-01, 3.8791e-01, 6.7097e-01,\n",
      "          8.0228e-01, 8.7406e-01, 9.9086e-01, 4.2056e-01, 6.9681e-01,\n",
      "          8.2036e-01, 7.3772e-01],\n",
      "         [4.2671e-01, 3.4217e-01, 6.1757e-01, 4.9223e-01, 1.2894e-01,\n",
      "          5.8767e-01, 7.0695e-01, 2.6891e-01, 3.1328e-01, 7.0984e-01,\n",
      "          7.1057e-01, 7.8905e-02]],\n",
      "\n",
      "        [[2.4335e-02, 8.4759e-01, 5.0354e-01, 1.7550e-01, 8.3577e-01,\n",
      "          9.3589e-01, 6.2399e-01, 3.3476e-01, 3.4461e-02, 1.9456e-01,\n",
      "          9.7765e-01, 8.8732e-01],\n",
      "         [9.6041e-01, 9.4025e-02, 9.5037e-01, 8.3888e-01, 9.7401e-01,\n",
      "          3.4857e-01, 4.2820e-01, 1.2538e-01, 5.1167e-01, 3.8812e-01,\n",
      "          9.2849e-01, 3.2165e-01],\n",
      "         [2.7928e-01, 1.3372e-03, 6.8252e-01, 5.0787e-01, 4.5061e-01,\n",
      "          5.3162e-01, 7.3599e-01, 2.1231e-01, 9.1532e-01, 2.8212e-01,\n",
      "          7.2789e-01, 6.3688e-01],\n",
      "         [8.6433e-01, 6.2183e-01, 5.0264e-01, 9.7311e-01, 2.7769e-01,\n",
      "          5.3631e-01, 5.2328e-01, 9.1907e-01, 9.2098e-01, 4.5857e-01,\n",
      "          9.6764e-01, 6.8599e-01],\n",
      "         [6.9070e-02, 5.4361e-01, 1.8248e-01, 4.3480e-01, 1.1012e-01,\n",
      "          3.8157e-01, 3.3787e-01, 5.5637e-01, 4.5258e-01, 1.7488e-01,\n",
      "          4.9520e-01, 8.6432e-01],\n",
      "         [4.5886e-01, 2.2617e-01, 7.1722e-01, 5.4964e-02, 1.1931e-01,\n",
      "          3.8107e-01, 3.6509e-01, 9.4730e-01, 5.2157e-01, 5.0104e-01,\n",
      "          7.7371e-02, 4.4938e-01],\n",
      "         [1.8223e-01, 9.3455e-01, 7.9311e-01, 9.2513e-01, 8.8136e-01,\n",
      "          9.3893e-01, 7.7692e-01, 6.8908e-01, 8.0205e-01, 3.4535e-01,\n",
      "          1.6590e-01, 7.4278e-01],\n",
      "         [1.9594e-01, 4.1082e-01, 4.7595e-01, 7.4318e-01, 3.8326e-01,\n",
      "          9.7903e-01, 2.4823e-01, 2.1815e-01, 1.2353e-01, 3.4795e-01,\n",
      "          2.0196e-01, 5.9322e-03],\n",
      "         [3.3362e-01, 4.9056e-01, 4.4320e-01, 6.0587e-01, 4.3871e-01,\n",
      "          6.2484e-01, 2.2548e-01, 4.8813e-01, 7.6841e-01, 7.8339e-01,\n",
      "          9.8281e-02, 9.1956e-02],\n",
      "         [9.2719e-01, 7.5582e-01, 4.9324e-01, 8.4887e-01, 4.0852e-01,\n",
      "          3.7266e-01, 4.5942e-01, 4.6105e-01, 8.7393e-01, 1.4220e-01,\n",
      "          9.1890e-01, 6.8974e-01],\n",
      "         [8.0040e-01, 9.4442e-02, 6.6976e-02, 6.7209e-01, 2.8355e-01,\n",
      "          3.9045e-01, 2.5804e-02, 6.4582e-01, 3.6609e-01, 8.8880e-01,\n",
      "          3.3692e-01, 3.3318e-01],\n",
      "         [3.3419e-01, 2.6277e-01, 3.5234e-01, 7.1394e-01, 2.6354e-02,\n",
      "          5.0490e-01, 5.8809e-01, 9.8467e-01, 3.7641e-01, 4.5863e-01,\n",
      "          5.8535e-01, 8.1723e-01]],\n",
      "\n",
      "        [[6.7470e-01, 8.7474e-01, 5.5967e-01, 1.0321e-01, 5.7861e-01,\n",
      "          1.4172e-01, 2.5148e-01, 1.9303e-01, 9.1242e-01, 4.1640e-02,\n",
      "          2.6987e-01, 6.9984e-01],\n",
      "         [6.7659e-01, 8.7451e-01, 5.9652e-01, 7.9132e-01, 8.6227e-01,\n",
      "          5.5402e-01, 5.9177e-01, 9.6435e-01, 7.1663e-01, 3.8702e-01,\n",
      "          5.0744e-01, 8.7232e-01],\n",
      "         [2.3890e-01, 3.4637e-01, 9.7432e-01, 4.7212e-01, 8.7668e-01,\n",
      "          8.6103e-01, 5.9668e-01, 7.1046e-02, 1.5667e-01, 7.1592e-01,\n",
      "          2.7037e-01, 6.6329e-01],\n",
      "         [1.4869e-02, 7.8592e-01, 5.0938e-01, 9.0416e-01, 8.9419e-01,\n",
      "          1.1224e-01, 4.1443e-01, 6.0249e-01, 4.4709e-01, 3.2959e-01,\n",
      "          3.4563e-01, 5.2110e-01],\n",
      "         [5.2429e-01, 2.3257e-01, 4.6375e-01, 6.0193e-01, 4.4395e-01,\n",
      "          1.3348e-01, 9.1415e-01, 9.2478e-01, 4.1294e-01, 9.1465e-01,\n",
      "          4.3388e-01, 1.4077e-01],\n",
      "         [7.2399e-01, 5.8650e-01, 1.9905e-01, 7.1087e-01, 3.7012e-01,\n",
      "          5.5661e-02, 5.6658e-01, 9.5799e-01, 2.4915e-01, 5.1039e-01,\n",
      "          2.2746e-02, 9.3177e-01],\n",
      "         [8.8452e-01, 5.5291e-01, 7.5162e-01, 5.3238e-01, 3.4851e-01,\n",
      "          5.8753e-01, 6.7433e-02, 1.4389e-01, 2.2736e-01, 2.4661e-01,\n",
      "          5.9412e-01, 3.7982e-01],\n",
      "         [6.4001e-01, 9.2347e-01, 3.8097e-01, 4.5561e-01, 7.8675e-01,\n",
      "          5.7948e-01, 2.9945e-01, 6.2332e-01, 1.7758e-01, 8.2892e-02,\n",
      "          5.9407e-01, 4.4757e-01],\n",
      "         [8.1583e-02, 9.2190e-01, 4.3404e-01, 2.5536e-01, 9.5562e-01,\n",
      "          9.2460e-01, 6.1629e-01, 1.4183e-01, 2.0771e-01, 7.7432e-01,\n",
      "          7.6524e-01, 5.3211e-01],\n",
      "         [2.5042e-02, 5.7974e-01, 9.6063e-01, 3.2023e-01, 9.6837e-01,\n",
      "          2.5511e-01, 6.6400e-01, 5.7140e-01, 3.5998e-01, 3.9244e-01,\n",
      "          7.1443e-01, 2.4265e-02],\n",
      "         [1.1535e-01, 1.0942e-01, 7.7460e-01, 2.5220e-01, 3.6116e-01,\n",
      "          5.7591e-01, 4.4653e-01, 6.3220e-01, 2.3933e-01, 3.4254e-03,\n",
      "          1.3716e-01, 4.9966e-01],\n",
      "         [7.2008e-01, 6.1888e-01, 5.5537e-02, 5.8458e-01, 5.6635e-01,\n",
      "          6.7584e-01, 3.9305e-01, 5.5131e-02, 1.7975e-01, 6.1808e-01,\n",
      "          9.5755e-01, 4.6466e-01]]])\n",
      "torch.Size([3, 12, 12])\n",
      "after flattening:\n",
      " tensor([[1.2036e-01, 3.5857e-01, 3.0722e-01, 6.0852e-01, 9.4305e-01, 2.7720e-01,\n",
      "         3.9667e-01, 4.7321e-01, 2.1927e-01, 9.6870e-01, 7.8115e-01, 9.9759e-01,\n",
      "         7.1010e-01, 6.7218e-01, 7.5192e-01, 4.6336e-02, 2.4287e-01, 1.6409e-01,\n",
      "         8.3532e-01, 8.4018e-02, 1.8201e-01, 5.0194e-01, 2.3946e-01, 4.7157e-01,\n",
      "         8.4941e-01, 8.7402e-01, 7.2627e-02, 3.0818e-02, 8.5465e-01, 6.8261e-01,\n",
      "         7.5643e-01, 2.6868e-01, 5.5701e-02, 3.1213e-01, 6.5913e-01, 6.4715e-01,\n",
      "         1.4572e-01, 5.0676e-01, 9.9729e-01, 8.4374e-01, 5.6391e-01, 9.9580e-01,\n",
      "         2.0392e-01, 9.2289e-01, 7.8074e-01, 7.0441e-01, 9.6406e-01, 5.1472e-01,\n",
      "         9.4192e-01, 1.2049e-01, 6.5736e-01, 2.2819e-02, 6.0878e-01, 3.2500e-01,\n",
      "         1.3713e-01, 6.0726e-01, 8.3132e-02, 1.7710e-01, 3.1849e-01, 3.0600e-01,\n",
      "         8.2168e-01, 5.6157e-02, 9.0684e-01, 1.5835e-01, 9.7183e-01, 8.5267e-02,\n",
      "         6.8410e-01, 8.3593e-01, 6.9281e-01, 5.4662e-01, 8.3735e-01, 5.0868e-01,\n",
      "         6.6205e-01, 2.8574e-01, 8.6120e-01, 4.1313e-01, 3.1117e-01, 1.5610e-01,\n",
      "         1.9948e-01, 9.3747e-01, 6.8569e-04, 4.7271e-02, 1.9549e-01, 8.5182e-03,\n",
      "         3.9558e-01, 7.9935e-03, 3.3891e-01, 8.6581e-02, 6.7433e-01, 9.3007e-01,\n",
      "         1.7772e-01, 6.1982e-01, 2.4283e-01, 5.4227e-03, 1.7955e-01, 4.4270e-02,\n",
      "         8.6561e-01, 8.4890e-01, 2.9216e-01, 2.9569e-01, 7.1616e-03, 5.2166e-01,\n",
      "         7.5104e-01, 2.2750e-01, 1.8223e-01, 7.3204e-01, 8.9928e-01, 9.9482e-01,\n",
      "         9.0702e-01, 9.7863e-01, 8.2605e-01, 8.2432e-01, 7.9553e-01, 1.8847e-01,\n",
      "         5.1431e-02, 5.2477e-01, 4.1782e-01, 1.0863e-01, 3.8186e-01, 5.0454e-01,\n",
      "         6.9354e-01, 3.3332e-01, 1.9284e-01, 3.8791e-01, 6.7097e-01, 8.0228e-01,\n",
      "         8.7406e-01, 9.9086e-01, 4.2056e-01, 6.9681e-01, 8.2036e-01, 7.3772e-01,\n",
      "         4.2671e-01, 3.4217e-01, 6.1757e-01, 4.9223e-01, 1.2894e-01, 5.8767e-01,\n",
      "         7.0695e-01, 2.6891e-01, 3.1328e-01, 7.0984e-01, 7.1057e-01, 7.8905e-02],\n",
      "        [2.4335e-02, 8.4759e-01, 5.0354e-01, 1.7550e-01, 8.3577e-01, 9.3589e-01,\n",
      "         6.2399e-01, 3.3476e-01, 3.4461e-02, 1.9456e-01, 9.7765e-01, 8.8732e-01,\n",
      "         9.6041e-01, 9.4025e-02, 9.5037e-01, 8.3888e-01, 9.7401e-01, 3.4857e-01,\n",
      "         4.2820e-01, 1.2538e-01, 5.1167e-01, 3.8812e-01, 9.2849e-01, 3.2165e-01,\n",
      "         2.7928e-01, 1.3372e-03, 6.8252e-01, 5.0787e-01, 4.5061e-01, 5.3162e-01,\n",
      "         7.3599e-01, 2.1231e-01, 9.1532e-01, 2.8212e-01, 7.2789e-01, 6.3688e-01,\n",
      "         8.6433e-01, 6.2183e-01, 5.0264e-01, 9.7311e-01, 2.7769e-01, 5.3631e-01,\n",
      "         5.2328e-01, 9.1907e-01, 9.2098e-01, 4.5857e-01, 9.6764e-01, 6.8599e-01,\n",
      "         6.9070e-02, 5.4361e-01, 1.8248e-01, 4.3480e-01, 1.1012e-01, 3.8157e-01,\n",
      "         3.3787e-01, 5.5637e-01, 4.5258e-01, 1.7488e-01, 4.9520e-01, 8.6432e-01,\n",
      "         4.5886e-01, 2.2617e-01, 7.1722e-01, 5.4964e-02, 1.1931e-01, 3.8107e-01,\n",
      "         3.6509e-01, 9.4730e-01, 5.2157e-01, 5.0104e-01, 7.7371e-02, 4.4938e-01,\n",
      "         1.8223e-01, 9.3455e-01, 7.9311e-01, 9.2513e-01, 8.8136e-01, 9.3893e-01,\n",
      "         7.7692e-01, 6.8908e-01, 8.0205e-01, 3.4535e-01, 1.6590e-01, 7.4278e-01,\n",
      "         1.9594e-01, 4.1082e-01, 4.7595e-01, 7.4318e-01, 3.8326e-01, 9.7903e-01,\n",
      "         2.4823e-01, 2.1815e-01, 1.2353e-01, 3.4795e-01, 2.0196e-01, 5.9322e-03,\n",
      "         3.3362e-01, 4.9056e-01, 4.4320e-01, 6.0587e-01, 4.3871e-01, 6.2484e-01,\n",
      "         2.2548e-01, 4.8813e-01, 7.6841e-01, 7.8339e-01, 9.8281e-02, 9.1956e-02,\n",
      "         9.2719e-01, 7.5582e-01, 4.9324e-01, 8.4887e-01, 4.0852e-01, 3.7266e-01,\n",
      "         4.5942e-01, 4.6105e-01, 8.7393e-01, 1.4220e-01, 9.1890e-01, 6.8974e-01,\n",
      "         8.0040e-01, 9.4442e-02, 6.6976e-02, 6.7209e-01, 2.8355e-01, 3.9045e-01,\n",
      "         2.5804e-02, 6.4582e-01, 3.6609e-01, 8.8880e-01, 3.3692e-01, 3.3318e-01,\n",
      "         3.3419e-01, 2.6277e-01, 3.5234e-01, 7.1394e-01, 2.6354e-02, 5.0490e-01,\n",
      "         5.8809e-01, 9.8467e-01, 3.7641e-01, 4.5863e-01, 5.8535e-01, 8.1723e-01],\n",
      "        [6.7470e-01, 8.7474e-01, 5.5967e-01, 1.0321e-01, 5.7861e-01, 1.4172e-01,\n",
      "         2.5148e-01, 1.9303e-01, 9.1242e-01, 4.1640e-02, 2.6987e-01, 6.9984e-01,\n",
      "         6.7659e-01, 8.7451e-01, 5.9652e-01, 7.9132e-01, 8.6227e-01, 5.5402e-01,\n",
      "         5.9177e-01, 9.6435e-01, 7.1663e-01, 3.8702e-01, 5.0744e-01, 8.7232e-01,\n",
      "         2.3890e-01, 3.4637e-01, 9.7432e-01, 4.7212e-01, 8.7668e-01, 8.6103e-01,\n",
      "         5.9668e-01, 7.1046e-02, 1.5667e-01, 7.1592e-01, 2.7037e-01, 6.6329e-01,\n",
      "         1.4869e-02, 7.8592e-01, 5.0938e-01, 9.0416e-01, 8.9419e-01, 1.1224e-01,\n",
      "         4.1443e-01, 6.0249e-01, 4.4709e-01, 3.2959e-01, 3.4563e-01, 5.2110e-01,\n",
      "         5.2429e-01, 2.3257e-01, 4.6375e-01, 6.0193e-01, 4.4395e-01, 1.3348e-01,\n",
      "         9.1415e-01, 9.2478e-01, 4.1294e-01, 9.1465e-01, 4.3388e-01, 1.4077e-01,\n",
      "         7.2399e-01, 5.8650e-01, 1.9905e-01, 7.1087e-01, 3.7012e-01, 5.5661e-02,\n",
      "         5.6658e-01, 9.5799e-01, 2.4915e-01, 5.1039e-01, 2.2746e-02, 9.3177e-01,\n",
      "         8.8452e-01, 5.5291e-01, 7.5162e-01, 5.3238e-01, 3.4851e-01, 5.8753e-01,\n",
      "         6.7433e-02, 1.4389e-01, 2.2736e-01, 2.4661e-01, 5.9412e-01, 3.7982e-01,\n",
      "         6.4001e-01, 9.2347e-01, 3.8097e-01, 4.5561e-01, 7.8675e-01, 5.7948e-01,\n",
      "         2.9945e-01, 6.2332e-01, 1.7758e-01, 8.2892e-02, 5.9407e-01, 4.4757e-01,\n",
      "         8.1583e-02, 9.2190e-01, 4.3404e-01, 2.5536e-01, 9.5562e-01, 9.2460e-01,\n",
      "         6.1629e-01, 1.4183e-01, 2.0771e-01, 7.7432e-01, 7.6524e-01, 5.3211e-01,\n",
      "         2.5042e-02, 5.7974e-01, 9.6063e-01, 3.2023e-01, 9.6837e-01, 2.5511e-01,\n",
      "         6.6400e-01, 5.7140e-01, 3.5998e-01, 3.9244e-01, 7.1443e-01, 2.4265e-02,\n",
      "         1.1535e-01, 1.0942e-01, 7.7460e-01, 2.5220e-01, 3.6116e-01, 5.7591e-01,\n",
      "         4.4653e-01, 6.3220e-01, 2.3933e-01, 3.4254e-03, 1.3716e-01, 4.9966e-01,\n",
      "         7.2008e-01, 6.1888e-01, 5.5537e-02, 5.8458e-01, 5.6635e-01, 6.7584e-01,\n",
      "         3.9305e-01, 5.5131e-02, 1.7975e-01, 6.1808e-01, 9.5755e-01, 4.6466e-01]])\n",
      "torch.Size([3, 144])\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Step-1:</b> input layer",
   "id": "fd5441d4774684f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:19.416325Z",
     "start_time": "2024-09-17T14:30:19.412741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Linear layer to apply linear transformation\n",
    "\n",
    "layer1 = nn.Linear(in_features=144, out_features=5)\n",
    "hidden1_input = layer1(flat_x)\n",
    "print(hidden1_input)\n",
    "print(hidden1_input.size())"
   ],
   "id": "f92a792fbcdfdd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3726,  0.3136, -0.7488,  0.4268,  0.0794],\n",
      "        [-0.2579,  0.0447, -0.4766,  0.3809,  0.1297],\n",
      "        [-0.1521, -0.1533, -0.4644,  0.0299, -0.2617]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:19.425561Z",
     "start_time": "2024-09-17T14:30:19.416997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## For a better understanding of the above cell:\n",
    "# \n",
    "# weights = torch.rand(144, 5)\n",
    "# print(\n",
    "#     f\"input matrix * random weights: {flat_x.size()}*{weights.size()}\"\n",
    "# )\n",
    "# hidden_layer_input = torch.matmul(input=flat_x, other=weights)\n",
    "# print(\"after mat_mul:\\n\", hidden_layer_input)"
   ],
   "id": "1c84675cb81f0e5",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Step-2:</b> Hidden layer 1",
   "id": "a54a0e5d7f908ae0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:19.436739Z",
     "start_time": "2024-09-17T14:30:19.426975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Before ReLU: {hidden1_input}\\n\\n\")\n",
    "hidden1 = nn.ReLU()\n",
    "hidden1_output = hidden1(hidden1_input)\n",
    "print(f\"After ReLU: {hidden1_output}\\n\\n\")"
   ],
   "id": "1d60b2ce95bda735",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.3726,  0.3136, -0.7488,  0.4268,  0.0794],\n",
      "        [-0.2579,  0.0447, -0.4766,  0.3809,  0.1297],\n",
      "        [-0.1521, -0.1533, -0.4644,  0.0299, -0.2617]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.3136, 0.0000, 0.4268, 0.0794],\n",
      "        [0.0000, 0.0447, 0.0000, 0.3809, 0.1297],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0299, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Step-3:</b> Hidden layer 2",
   "id": "4352d9320a124eda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:19.446901Z",
     "start_time": "2024-09-17T14:30:19.437741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Moving on to the last layer\n",
    "\n",
    "hidden2 = nn.Linear(in_features=5, out_features=3)\n",
    "hidden2_output = hidden2(hidden1_output)\n",
    "print(hidden2_output.size())\n",
    "print(f\"output matrix: {hidden2_output}\\n\\n\")"
   ],
   "id": "df9f14516ca67ea3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "output matrix: tensor([[ 0.4140,  0.2501, -0.1245],\n",
      "        [ 0.4198,  0.1970, -0.0360],\n",
      "        [ 0.3581,  0.0234, -0.1719]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Step-3.5:</b> Logits ",
   "id": "8b46f46291d109b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:19.456053Z",
     "start_time": "2024-09-17T14:30:19.447999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = hidden2_output # logits are basically the output before going through the last activation function\n",
    "print(logits.size())\n",
    "print(logits)"
   ],
   "id": "909a0a03e89ad467",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[ 0.4140,  0.2501, -0.1245],\n",
      "        [ 0.4198,  0.1970, -0.0360],\n",
      "        [ 0.3581,  0.0234, -0.1719]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Step-4:</b> Output Layer with softmax activation ",
   "id": "12163681c86ea9c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:19.466530Z",
     "start_time": "2024-09-17T14:30:19.456828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Last activation: Softmax\n",
    "# Why softmax?\n",
    "# because, he last linear layer of the neural network returns logits [-infty, infty], softmax scales the values/logits to [0,1]\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "soft_out = softmax(logits)\n",
    "\n",
    "print(f\"Before softmax: {logits}\\nsize: {logits.size()}\\n\")\n",
    "print(f\"after softmax: {soft_out}\\nsize: {soft_out.size()}\\n\")"
   ],
   "id": "6f038229f7b3c999",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before softmax: tensor([[ 0.4140,  0.2501, -0.1245],\n",
      "        [ 0.4198,  0.1970, -0.0360],\n",
      "        [ 0.3581,  0.0234, -0.1719]], grad_fn=<AddmmBackward0>)\n",
      "size: torch.Size([3, 3])\n",
      "\n",
      "after softmax: tensor([[0.4111, 0.3490, 0.2399],\n",
      "        [0.4108, 0.3288, 0.2604],\n",
      "        [0.4340, 0.3105, 0.2555]], grad_fn=<SoftmaxBackward0>)\n",
      "size: torch.Size([3, 3])\n",
      "\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### All together:",
   "id": "e032e38fa01773be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:30:19.476499Z",
     "start_time": "2024-09-17T14:30:19.467303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=144, out_features=5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 3),\n",
    "    nn.Softmax(dim=1)\n",
    "\n",
    ")\n",
    "\n",
    "output = seq_modules(X)\n",
    "print(output.size())\n",
    "print(output)"
   ],
   "id": "1cb170bda83d3fec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[0.3864, 0.2482, 0.3654],\n",
      "        [0.3831, 0.2565, 0.3604],\n",
      "        [0.3935, 0.2610, 0.3455]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
